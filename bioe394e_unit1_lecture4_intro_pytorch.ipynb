{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eacf42e7",
      "metadata": {
        "id": "eacf42e7"
      },
      "source": [
        "# **Introduction to PyTorch** - Logistic Regression, MLP, and MNIST\n",
        "\n",
        "**Introduction to Deep Learning (BioE 394E)**  \n",
        "**Code by Billy Carson (Duke University)**  \n",
        "\n",
        "> In this notebook, we're going to gain familiarity with the PyTorch deep learning framework by building PyTorch models to perform handwritten digit classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wMmJqJsll8j6",
      "metadata": {
        "id": "wMmJqJsll8j6"
      },
      "source": [
        "## **0. Downloading Data**\n",
        "\n",
        "In this notwbook we will use the MNIST dataset, which we download below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t6yxlQCPmIBA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6yxlQCPmIBA",
        "outputId": "a7004f3a-b571-443c-9098-cc24e98f0f5c"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/rhenaog/bioe394e/main/mnist/train_images.npy -P mnist\n",
        "!wget https://raw.githubusercontent.com/rhenaog/bioe394e/main/mnist/test_images.npy -P mnist\n",
        "!wget https://raw.githubusercontent.com/rhenaog/bioe394e/main/mnist/train_labels.npy -P mnist\n",
        "!wget https://raw.githubusercontent.com/rhenaog/bioe394e/main/mnist/test_labels.npy -P mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073815c6",
      "metadata": {
        "id": "073815c6"
      },
      "source": [
        "## **1. Setting Up the Enivronment**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f60e80c",
      "metadata": {
        "id": "1f60e80c"
      },
      "source": [
        "### 1.1. Import utilities\n",
        "\n",
        "First, we need to import necessary libraries, modules, and functions we'll use throughout this tutorial. To give ourselves access to the offerings of the PyTorch library, we'll import the [torch](https://pytorch.org/docs/stable/torch.html) package and the [torch.nn](https://pytorch.org/docs/stable/nn.html) module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdd4146f",
      "metadata": {
        "id": "fdd4146f"
      },
      "outputs": [],
      "source": [
        "# Define random state\n",
        "random_state = 0\n",
        "\n",
        "# Import libraries, modules, and functions\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a05cde",
      "metadata": {
        "id": "39a05cde"
      },
      "source": [
        "### 1.2. GPU/CUDA availability\n",
        "\n",
        "When setting up your coding environment for a project or experiment, it's important that we are aware of the available computing resources. The resources we have access to will determine what models we can or can't use and the amount of data we can load into memory, among other considerations. For this tutorial, since we are dealing with simpler machine learning models, we won't need access to a [Graphics Processing Unit](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU). GPUs help speed up the training of deep learning models by parallelizing many computations intead of computing sequentially. This parallelization is critical when training more complex models like [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs), and without it these models would take prohibitively long to train.\n",
        "\n",
        "We can check whether or not our environment has access to a GPU by calling the [`torch.cuda.is_available()`](https://pytorch.org/docs/stable/cuda.html) function. [CUDA](https://developer.nvidia.com/about-cuda) is a parallel computing platform and programming model developed by Nvidia for general computing on its own GPUs. This function will tell us if CUDA is available in this computing environment. Since we don't need/have access to a GPU, `torch.cuda.is_available()` should return `False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d17414",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0d17414",
        "outputId": "cfa5e308-b2f3-49a2-f749-463b71c3fcb9"
      },
      "outputs": [],
      "source": [
        "# Display GPU/CUDA availability\n",
        "print('\\nCUDA availability:  %s\\n' % (torch.cuda.is_available()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "804bdbfe",
      "metadata": {
        "id": "804bdbfe"
      },
      "source": [
        "## **2. Loading and Preparing the Data**\n",
        "\n",
        "One of the first (and often most important) steps in a machine learning pipeline is preparing the data. Here, we will load the MNIST dataset of images of handwritten digits and format this data so it can be used with PyTorch models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5f0236",
      "metadata": {
        "id": "0a5f0236"
      },
      "source": [
        "### 2.1. Load MNIST dataset\n",
        "\n",
        "The [MNIST dataset](http://yann.lecun.com/exdb/mnist/) is very popular machine learning dataset, consisting of 70,000 grayscale, 28x28 images of handwritten digits.  We'll be using it as our example dataset for this section of the tutorial, with the goal being to predict which digit is depicted in each image.\n",
        "\n",
        "<img src=\"https://github.com/rhenaog/bioe394e/blob/main/files/mnist_examples.png?raw=1\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "First, let's load the training and test sets of MNIST. The MNIST dataset needs to be converted to a type that can be operated on by PyTorch models. For this tutorial, MNIST data has been saved as NumPy arrays in binary files. [NumPy](https://numpy.org) is a commonly-used Python library which provides support for multi-dimensional arrays and matrices, along with a variety of mathematical functions to operate on these arrays. In Python data science and machine learning pipelines, NumPy arrays are commonly used to store data and facilitate quick and efficient operations on array-like data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7550296f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7550296f",
        "outputId": "3220e489-7137-4a3d-af33-0df19f178e9f"
      },
      "outputs": [],
      "source": [
        "# Load NumPy arrays of MNIST dataset\n",
        "mnist_data_dir = 'mnist/'\n",
        "X_raw_train_npy = np.load(mnist_data_dir + 'train_images.npy')\n",
        "y_train_npy = np.load(mnist_data_dir + 'train_labels.npy')\n",
        "X_raw_test_npy = np.load(mnist_data_dir + 'test_images.npy')\n",
        "y_test_npy = np.load(mnist_data_dir + 'test_labels.npy')\n",
        "\n",
        "# Display class type\n",
        "print('\\nClass types:')\n",
        "print('  Images:  %s' % (type(X_raw_train_npy)))\n",
        "print('  Labels:  %s' % (type(y_train_npy)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60b9c9c",
      "metadata": {
        "id": "c60b9c9c"
      },
      "source": [
        "### 2.2 Convert data from NumPy arrays to PyTorch tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69e762b6",
      "metadata": {
        "id": "69e762b6"
      },
      "source": [
        "Now, MNIST data stored as NumPy arrays must be converted to [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) so that this data can be operated on by PyTorch models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576875ec",
      "metadata": {
        "id": "576875ec"
      },
      "outputs": [],
      "source": [
        "# Load NumPy arrays of MNIST dataset\n",
        "X_raw_train = torch.from_numpy(X_raw_train_npy)\n",
        "y_train = torch.from_numpy(y_train_npy)\n",
        "X_raw_test = torch.from_numpy(X_raw_test_npy)\n",
        "y_test = torch.from_numpy(y_test_npy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9cca7bb",
      "metadata": {
        "id": "d9cca7bb"
      },
      "source": [
        "### 2.3. Explore data/PyTorch tensors\n",
        "\n",
        "Tensors are a specialized data structure very similar to arrays and matrices that house data in $N$ dimensions. Tensors can be thought of as generalizations of matrices (which are specifically 2-dimensional tensors) to $N$-dimensional space. \n",
        "\n",
        "In PyTorch, tensors are used to encode data, model inputs, model outputs, and model parameters. [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) are objects similar to [NumPy ndarrays](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html), except that tensors can run on GPUs or other hardware accelerators. Tensors are also optimized for automatic differentiation with [`autograd`](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html), PyTorch’s automatic differentiation engine that powers neural network training. Here, where call the tensor method `shape()` to get an idea of the size and dimensionality of our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44531dd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44531dd1",
        "outputId": "c557789c-7abf-4b0f-f263-fbf864201e12"
      },
      "outputs": [],
      "source": [
        "# Display training data dimensionality\n",
        "print('\\nTraining data dimensionality:')\n",
        "print('  Images:  (%d, %d)' % (X_raw_train.shape))\n",
        "print('  Labels:  (%d, %d)' % (y_train.shape))\n",
        "\n",
        "# Display test data dimensionality\n",
        "print('\\nTest data dimensionality:')\n",
        "print('  Images:  (%d, %d)' % (X_raw_test.shape))\n",
        "print('  Labels:  (%d, %d)\\n' % (y_test.shape))\n",
        "\n",
        "# Display different labels\n",
        "print('Training labels:')\n",
        "print(np.unique(y_train.numpy()))\n",
        "print('\\nTest labels:')\n",
        "print(np.unique(y_test.numpy()))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5270fca7",
      "metadata": {
        "id": "5270fca7"
      },
      "source": [
        "### 2.4. Format tensors\n",
        "\n",
        "For our PyTorch models and loss functions to be able to operate on the tensors of MNIST data, they first need to be of a specific [tensor type](https://pytorch.org/docs/stable/tensors.html). Specifically, the image data tensors need to be 32-bit floating point tensors (tensor type `FloatTensor`) and the labels tensors need to be 64-bit signed integers (tensor type `LongTensor`).\n",
        "\n",
        "We can display the tensor type by calling the tensor method `type()`, which will return the tensor type. Our image data tensors need to be converted from type to type `FloatTensor` and our labels tensors need to be converted to type `LongTensor`. We can make this conversion by calling the tensor method `float()` and `long()`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53494ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c53494ef",
        "outputId": "764dfef8-052a-47f1-9397-79cc305fbe4d"
      },
      "outputs": [],
      "source": [
        "# Display original tensor type\n",
        "print('\\nTensor type (before conversion):')\n",
        "print('  Images: %s' % (X_raw_train.type()))\n",
        "print('  Labels: %s\\n' % (y_train.type()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfdf1fa",
      "metadata": {
        "id": "3cfdf1fa"
      },
      "outputs": [],
      "source": [
        "# Convert image tensor type to float and labels tensor type to long\n",
        "X_raw_train = X_raw_train.float()\n",
        "X_raw_test = X_raw_test.float()\n",
        "y_train = y_train.long()\n",
        "y_test = y_test.long()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17dad6ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17dad6ef",
        "outputId": "4784a95c-5f3e-4bc7-db65-a1c58cee091e"
      },
      "outputs": [],
      "source": [
        "# Display tensor type after conversion\n",
        "print('\\nTensor type (after conversion):')\n",
        "print('  Images: %s' % (X_raw_train.type()))\n",
        "print('  Labels: %s\\n' % (y_train.type()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfeed4e",
      "metadata": {
        "id": "5cfeed4e"
      },
      "source": [
        "### 2.5. Scale data\n",
        "\n",
        "Often in many machine learning and data science applications, it helps to first scale the data. Scaling our data can ensure that all of our features are on approximately the same scale. Significant differences in the scales across input variables may increase the difficulty of our optimization problem [1].\n",
        "\n",
        "For this modelling problem, we'll scale our data to be between 0 and 1. Since MNIST data are black and white images, data elements correspond to pixel intensities with values between 0 and 255. Therefore, to scale our data between 0 and 1 we just need to divide our image data tensors by 255.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72057f27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72057f27",
        "outputId": "5a6cbb6a-d986-4634-ffef-81eac9851fbb"
      },
      "outputs": [],
      "source": [
        "# Display data max and min\n",
        "print('\\nData range pre-scaling (min, max)):')\n",
        "print('  Train images:  %.1f, %.1f' % (torch.min(X_raw_train), torch.max(X_raw_train)))\n",
        "print('  Test images:  %.1f, %.1f\\n' % (torch.min(X_raw_test), torch.max(X_raw_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf42b8e",
      "metadata": {
        "id": "6bf42b8e"
      },
      "outputs": [],
      "source": [
        "# Scale image pixel intensities between 0 and 1\n",
        "X_train = X_raw_train / 255.0\n",
        "X_test = X_raw_test / 255.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a76f8be6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76f8be6",
        "outputId": "2f1dcb4f-ae2e-4f0f-9dd1-f76de1dbd149"
      },
      "outputs": [],
      "source": [
        "# Display data max and min\n",
        "print('\\nData range post-scaling (min, max)):')\n",
        "print('  Train images:  %.1f, %.1f' % (torch.min(X_train), torch.max(X_train)))\n",
        "print('  Test images:  %.1f, %.1f\\n' % (torch.min(X_test), torch.max(X_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac9d3d4",
      "metadata": {
        "id": "4ac9d3d4"
      },
      "source": [
        "### 2.6. Reshape data\n",
        "\n",
        "Sometimes we need to reshape the data so that our chosen model can operate on it. Computer vision models such as CNNs operate on directly 2-dimensional image data, so if we were using a CNN we would not need to reshape our MNIST data. However, for this tutorial we will be working with logistic regression and MLP models. Both of these models require input data to be a 1-dimensional vector, meanining that we'll have to \"flatten\" our image data.\n",
        "\n",
        "When flattening the image data, we want to preserve the first dimension of our tensors, which represents the samples or observations dimension. We can reshape our training and test data tensors using the [`view()`](https://pytorch.org/docs/stable/tensor_view.html) method. Additionally, this method avoids explicit data copy, thus facilitating fast and memory efficient reshaping, slicing, and element-wise operations on tensors [2].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e9ba278",
      "metadata": {
        "id": "6e9ba278"
      },
      "outputs": [],
      "source": [
        "# Reshape and \"flatten\" image data\n",
        "# Hint: -1 tells view() to infer the size of a given dimension\n",
        "X_train = X_train.view(X_train.shape[0], -1)\n",
        "X_test = X_test.view(X_test.shape[0], -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06220821",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06220821",
        "outputId": "79307650-1a9e-4faf-ab19-980993db306b"
      },
      "outputs": [],
      "source": [
        "# Display training data dimensionality\n",
        "print('\\nTraining data dimensionality:')\n",
        "print('  Images:  (%d, %d)' % (X_train.shape))\n",
        "\n",
        "# Display test data dimensionality\n",
        "print('\\nTest data dimensionality:')\n",
        "print('  Images:  (%d, %d)\\n' % (X_test.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82b4bc92",
      "metadata": {
        "id": "82b4bc92"
      },
      "source": [
        "### 2.7. Define PyTorch dataset class\n",
        "\n",
        "[`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is an abstract class representing a dataset. It allows us to treat the dataset as an object of a class, rather than a set of data and labels. Creating a custom dataset will allow us to have greater control over how our model loads and interacts with the data, as well as permit built-in batching, shuffling, and loading data in parallel later on [3]. A custom `Dataset` class must implement three functions: ` __init__`, `__len__`, and `__getitem__`:\n",
        "\n",
        "* The `__init__` function is run once when instantiating the `Dataset` object.\n",
        "* The `__len__` function returns the number of samples in our dataset.\n",
        "* The `__getitem__` function loads and returns a sample from the dataset at the given index idx.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb00ede",
      "metadata": {
        "id": "2cb00ede"
      },
      "outputs": [],
      "source": [
        "# MNIST PyTorch dataset class definition\n",
        "class MNISTDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch dataset object for the MNIST dataset.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : torch.Tensor\n",
        "        Flattened image data of size (n_samples x 784).\n",
        "    y : torch.Tensor\n",
        "        Image labels, integer values 0, 1, 2, ..., 9.\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    X : torch.Tensor\n",
        "        Flattened image data of size (n_samples x 784).\n",
        "    y : torch.Tensor\n",
        "        Image labels, integer values 0, 1, 2, ..., 9.\n",
        "    len : int\n",
        "        Length of dataset/number of observations in dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    # MNIST PyTorch dataset instantiation method\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        MNIST dataset instantiation method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : torch.Tensor\n",
        "            Flattened image data of size (n_samples x 784).\n",
        "        y : torch.Tensor\n",
        "            Image labels, integer values 0, 1, 2, ..., 9.\n",
        "        \n",
        "        Attributes\n",
        "        ----------\n",
        "        X : torch.Tensor\n",
        "            Flattened image data of size (n_samples x 784).\n",
        "        y : torch.Tensor\n",
        "            Image labels, integer values 0, 1, 2, ..., 9.\n",
        "        len : int\n",
        "            Length of dataset/number of observations in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Assign attributes\n",
        "        self.X = X             # flattened image data\n",
        "        self.y = y             # image labels\n",
        "        self.len = X.shape[0]  # length of dataset\n",
        "\n",
        "    # Dataset length method\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Method that returns length of dataset/number of observations in dataset.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        self.len : int\n",
        "            Length of dataset/number of observations in dataset.\n",
        "        \"\"\"\n",
        "        \n",
        "        return self.len\n",
        "\n",
        "    # Dataset indexing method\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Method that retrieves samples and corresponding labels from the dataset.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Integer index of dataset sample to be retrieved.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        X_idx : torch.Tensor\n",
        "            Image sample.\n",
        "        y_idx : torch.Tensor\n",
        "            Label corresponding to image sample.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Retrieve sample of data indexed by idx\n",
        "        X_idx = self.X[idx, :]\n",
        "        y_idx = self.y[idx]\n",
        "\n",
        "        # Return data sample\n",
        "        return X_idx, y_idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4844b6f",
      "metadata": {
        "id": "d4844b6f"
      },
      "source": [
        "### 2.8. Instantiate PyTorch datasets\n",
        "\n",
        "Here, we instantiate a training and test version of the `MNISTDataset` class we defined above. Instantiating a class creates a copy or instance of the class which inherits all class attributes and methods. To instantiate an object in Python, we simply call the class as if it were a function, passing the requisite arguments to the `__init__` method. The newly created object will then be assigned to whatever variable name we choose.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931affe9",
      "metadata": {
        "id": "931affe9"
      },
      "outputs": [],
      "source": [
        "# Instantiate MNIST training and test PyTorch dataset objects\n",
        "train_dataset = MNISTDataset(X_train, y_train)\n",
        "test_dataset = MNISTDataset(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c586b9a9",
      "metadata": {
        "id": "c586b9a9"
      },
      "source": [
        "### 2.9. Instantiate PyTorch data loader objects\n",
        "\n",
        "While we could interface directly with the data as a PyTorch `torch.utils.data.Dataset` object, it's often easier to use a PyTorch [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) object. The PyTorch `DataLoader` is an iterator that automatically takes care of:\n",
        "* batching the data\n",
        "* shuffling the data\n",
        "* loading the data in parallel using multiprocessing workers\n",
        "\n",
        "Similar to how we created both training and test set instances of PyTorch `Dataset` objects, we will now create instances of training and test set `DataLoader` objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24690dc",
      "metadata": {
        "id": "d24690dc"
      },
      "outputs": [],
      "source": [
        "# Instantiate MNIST training and test PyTorch data loader objects\n",
        "batch_size = 100\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f505bb",
      "metadata": {
        "id": "26f505bb"
      },
      "source": [
        "## **3. Building a Logistic Regression Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05fe9b28",
      "metadata": {
        "id": "05fe9b28"
      },
      "source": [
        "### 3.1. Define logistic regression model\n",
        "\n",
        "For the first part of this tutorial, we'll be building a logistic regression model, which is essentially a fully-connected neural network without any hidden layers. Although basic, logistic regression can perform quite well on many simple classification tasks.\n",
        "\n",
        "PyTorch has higher level abstractions to help speed up implementation and improve model organization. While there are many ways to organize PyTorch code, one common paradigm is the `torch.nn.Module`. PyTorch uses modules to represent neural networks. Modules are:\n",
        "\n",
        "* Building blocks of stateful computation. PyTorch provides a robust library of pre-defined modules and makes it simple to define new custom modules, facilitating construction of many different deep neural network architecturess.\n",
        "* Tightly integrated with PyTorch’s autograd system. Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.\n",
        "* Easy to work with and transform. Modules are straightforward to save, restore, transfer between CPU/GPU/TPU devices, and more [4].\n",
        "\n",
        "For more on `torch.nn.Module`, see [this](https://pytorch.org/tutorials/beginner/nn_tutorial.html) tutorial. To create our LogisticRegression model that inherits from `torch.nn.Module`, we need to define two methods: `__init__()` and `forward()`. `__init__()` initializes the model parameters and assigns layers to object attributes. The `forward()` method computes the forward pass of the model by passing minibatches of input data through the different layers of the model.\n",
        "\n",
        "To get an idea of what types of PyTorch layers to include in our logistic regression models, let's take a look at the mathematical formulation of logistic regression. Multiclass logistic regression can be expressed in matrix form as:\n",
        "\n",
        "\\begin{align}\n",
        "y = x W + b \n",
        "\\end{align}\n",
        "\n",
        "To take advantage of parallel computation, we commonly process multiple inputs $x$ at once, in a minibatch. We can individual observations or samples of our data $x$ into a batch matrix of $X$. This can be expressed in matrix form as:\n",
        "\n",
        "\\begin{align}\n",
        "Y = X W + b \n",
        "\\end{align}\n",
        "\n",
        "In our specific example, the minibatch size $m$ is 100, the dimension of the data is 28 $\\times$ 28 = 784, and the number of classes $c$ is 10. Below is an image depicting the transformation of a batch of data $X$ into a set of corresponding predictions $Y$:\n",
        "\n",
        "<img src=\"https://github.com/rhenaog/bioe394e/blob/main/files/mnist_matmul.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "PyTorch offers a pre-defined [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer that computes a linear transformation of the input data. We can use this layer to define the parameters of our logistic regression model. Typically, after the `Linear` layer we would implement a softmax activation function that converts our output to a multinomial probability distribution. However, PyTorch has a way of building this calculation into its loss functions. This helps model stability during training. Therefore, we won't add a softmax activation function for our models and instead leave that calculation to PyTorch's loss functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8393a4ee",
      "metadata": {
        "id": "8393a4ee"
      },
      "outputs": [],
      "source": [
        "# Logistic regression model class definition\n",
        "class LogisticRegression(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of a logistic regression model.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dim : int\n",
        "        Size/dimensionality of the model input data.\n",
        "    output_dim : int\n",
        "        Size/dimensionality of the model output.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    input_dim : int\n",
        "        Size/dimensionality of the model input data.\n",
        "    output_dim : int\n",
        "        Size/dimensionality of the model output.\n",
        "    linear : torch.nn.Linear\n",
        "        Fully-connected/linear layer.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Logistic regression instantiation method\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Logistic regression model instantiation method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Size/dimensionality of the model input data.\n",
        "        output_dim : int\n",
        "            Size/dimensionality of the model output.\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Size/dimensionality of the model input data.\n",
        "        output_dim : int\n",
        "            Size/dimensionality of the model output.\n",
        "        linear : torch.nn.Linear\n",
        "            Fully-connected/linear layer.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Inherit from torch.nn.Module\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        \n",
        "        # Assign logistic regression parameters to model attributes\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.linear = nn.Linear(\n",
        "            in_features=input_dim,\n",
        "            out_features=output_dim,\n",
        "            bias=True)\n",
        "    \n",
        "    # Logistic regression forward pass method\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Logistic regression forward pass method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor()\n",
        "            Tensor of input data.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        logits : torch.Tensor()\n",
        "             Raw model predictions (not passed through sigmoid or softmax function).\n",
        "        \"\"\"\n",
        "        \n",
        "        # Calculate model predictions/logits\n",
        "        logits = self.linear(x)\n",
        "        \n",
        "        # Return model output\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e27f8db",
      "metadata": {
        "id": "3e27f8db"
      },
      "source": [
        "### 3.2. Instantiate model\n",
        "\n",
        "To instantiate our logistic regression model, we need to pass two parameters to the instantiation method `__init__()`: the model `input_dim` and model `output_dim`. `input_dim` will be the dimensionality of our flattened image data, which is 784. `output_dim` corresponds to the number of digits we are trying to classify, which is 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca70b11f",
      "metadata": {
        "id": "ca70b11f"
      },
      "outputs": [],
      "source": [
        "# Define model input and output dimensions from data\n",
        "input_dim = X_train.shape[1]                # second dimension is data dimensionality\n",
        "output_dim = len(torch.unique(y_train))  # output dim. is equal to number of unique labels\n",
        "\n",
        "# Instantiate logistic regression model\n",
        "lr_model = LogisticRegression(input_dim=input_dim, output_dim=output_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79da27af",
      "metadata": {
        "id": "79da27af"
      },
      "source": [
        "## **4. Training the Logistic Regression Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "488fecf7",
      "metadata": {
        "id": "488fecf7"
      },
      "source": [
        "### 4.1. Defining a loss function\n",
        "\n",
        "We need a way to evaluate our model predictions. This evaluation is done via a loss function, which takes the model's predictions and returns a single number summarizing model performance. The magnitude of this loss will inform how much or how little we should update the model parameters. The loss we commonly use in classification is cross-entropy, a concept from information theory. Cross-entropy not only captures whether or not model predictions are correct, it also accounts for how *confident* these predictions are. This encourages the model to produce very high probabilities for correct answers while driving down the probabilities for the wrong answers.\n",
        "\n",
        "To define our loss function object, we can use PyTorch's implementation of cross entropy [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss). In addition to computing the cross entropy loss, this loss function takes logits as inputs computes the log-softmax internally, which is why we did not have to add a softmax activation at the end of our logistic regression implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5b369d",
      "metadata": {
        "id": "3c5b369d"
      },
      "outputs": [],
      "source": [
        "# Instantiate cross-entropy loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e3a7752",
      "metadata": {
        "id": "9e3a7752"
      },
      "source": [
        "### 4.2. Defining optimizer and learning rate\n",
        "\n",
        "Now that we've defined our loss function as a way of quantifying how good or bad our model predictions are, we can improve our model by changing the parameters in a way that minimizes the loss. For neural networks, this is done via [backpropagation](https://en.wikipedia.org/wiki/Backpropagation): we take the gradient of the loss with respect to model parameters and take a \"step\" in the direction that reduces our loss. If we were not using a deep learning framework like PyTorch, we would have to go through and derive all the gradients ourselves by hand, then code them into our training pipeline. Luckily, Pytorch has automated gradient calculation with [`torch.autograd`](https://pytorch.org/docs/stable/autograd.html): an \"automatic differentiation engine that powers neural network training\" [5].\n",
        "\n",
        "Next, we define our optimizer and the learning rate we'll use for training the model. [`torch.optim`](https://pytorch.org/docs/stable/optim.html) is a package of various, commonly-used optimization algorithms implementations. A `torch.optim` optimizer object will keep track of the current state of model parameters and update the parameters based on the gradients computed by `torch.autograd`. To construct an optimizer you must provide an iterable containing the model parameters to optimize. This can be done by calling the `parameters()` method of our model (a method that is inherited from `torch.nn.Module`). Then, you can specify optimizer-specific options such as the learning rate and weight decay. For many deep learning applications, standard [stochastic gradient descent](https://scikit-learn.org/stable/modules/sgd.html) (SGD) optimizes models during training quite well. We can use PyTorch's `torch.optim` implementation of SGD as our optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f681ed01",
      "metadata": {
        "id": "f681ed01"
      },
      "outputs": [],
      "source": [
        "# Instantiate stochastic gradient descent (SGD) optimizer\n",
        "lr = 0.1\n",
        "optimizer = torch.optim.SGD(lr_model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10fa4a94",
      "metadata": {
        "id": "10fa4a94"
      },
      "source": [
        "### 4.3. Define number of training epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86dba600",
      "metadata": {
        "id": "86dba600"
      },
      "outputs": [],
      "source": [
        "# Number of training epochs (epochs = # of passes through data)\n",
        "n_epochs = 20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebfd8d35",
      "metadata": {
        "id": "ebfd8d35"
      },
      "source": [
        "### Training logistic regression on MNIST\n",
        "\n",
        "To train our model, we need to repeat the following steps:\n",
        "\n",
        "1. Draw a minibatch.\n",
        "2. Reset all gradients to zero in the buffers.\n",
        "3. Perform the forward pass (compute prediction, calculate loss).\n",
        "4. Perform the backward pass (compute gradients with respects to loss, optimize parameters with SGD).\n",
        "\n",
        "Performing these steps for the entirety of a dataset once is referred to as an epoch. We'll accomplish the above steps using the following PyTorch functionalities:\n",
        "\n",
        "1. Draw a minibatches by iterating over our training `DataLoader` instance. Remember, our custom PyTorch data loader takes care of batching the data, shuffling the data, and loading the data in parallel.\n",
        "2. Reset all gradients to zero in the buffers using the `zero_grad()` method of our optimizer. We need to set the gradients to zero before performing backpropragation because PyTorch accumulates the gradients on subsequent backward passes if `zero_grad()` is not called. Accumulation of gradients is convenient for models such as recurrent neural networks (RNNs); however, for this tutorial we'll zero out gradients before processing each minibatch of data.\n",
        "3. Perform the forward pass by providing the minibatch of data to our model's `forward()` method.\n",
        "4. Perform the backward pass by calling the `backward()` method of our model's computed loss and the `step()` method of our optimizer. The `backward()` method of our model computes the parameter gradients with respects to the calculated loss, and the `step()` method of our optimizer updates the parameters according to the calculated gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e551991d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e551991d",
        "outputId": "e9cf4c75-122f-4f62-f35c-092e0cbf85b8"
      },
      "outputs": [],
      "source": [
        "# Get weights\n",
        "weights_before = lr_model.linear.weight.detach().numpy().copy()\n",
        "\n",
        "# Time model training\n",
        "print('\\nTraining logistic regression model...\\n')\n",
        "time_start = time.time()\n",
        "\n",
        "# Place model in training mode\n",
        "# .train() method affects operations such as dropout and batch normalization\n",
        "lr_model.train()\n",
        "\n",
        "# Train model/iterate over epochs\n",
        "for epoch in range(n_epochs):\n",
        "    # Initialize epoch metrics variables\n",
        "    n_obs = 0\n",
        "    loss_sum = 0\n",
        "    n_correct = 0\n",
        "    \n",
        "    # Iterate through training data mini-batchesbatches\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # Zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        y_batch = y_batch.squeeze(1)\n",
        "        y_pred = lr_model(X_batch)         # model predictions\n",
        "        loss = loss_func(y_pred, y_batch)  # loss function evaluation\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()    # backpropagation\n",
        "        optimizer.step()   # update parameters according to learning rate, gradients\n",
        "        \n",
        "        # Update epoch metrics variables\n",
        "        n_batch = len(y_batch)\n",
        "        n_obs += n_batch\n",
        "        loss_sum += n_batch * loss.item()\n",
        "        n_correct += (y_pred.argmax(1) == y_batch.long()).float().sum().item()\n",
        "    \n",
        "    # Calculate epoch training loss and training accuracy\n",
        "    loss_train = loss_sum / n_obs\n",
        "    acc_train = n_correct / n_obs\n",
        "    \n",
        "    # Display training progress\n",
        "    prog_disp_freq = 1   # frequency of training progress display\n",
        "    if (epoch + 1) % prog_disp_freq == 0 or epoch == 0 or (epoch + 1) == n_epochs:\n",
        "        print('  E%02d | train loss: %s | train acc.: %s' % \n",
        "              (epoch + 1, '{:.4f}'.format(loss_train), '{:.4f}'.format(acc_train)))\n",
        "\n",
        "# Time model training\n",
        "time_end = time.time()\n",
        "print('\\nLogistic regression model training complete.\\n')\n",
        "print('Time to train logistic regression model:  %.1f s\\n' % (time_end - time_start))\n",
        "\n",
        "# Get weights after training\n",
        "weights_after = lr_model.linear.weight.detach().numpy().copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a67302b",
      "metadata": {
        "id": "0a67302b"
      },
      "source": [
        "### 4.4. Visualize logistic regression weights\n",
        "\n",
        "One of the nice things about logistic regression is that since it is a linear model we can interpret what the model has learned by plotting the weights before and after training.\n",
        "\n",
        "Before training, the model weights for each digit/output look like a static TV screen. Since the weights were randomly initialized, it is expected that the model parameters will appear random before training on any data. After training, we can see the model learned a general template for each digit. Remember that logistic regression takes a dot product between the weights of each digit and input. Therefore, the more the input matches the template or weights for a given digit, the higher the value of the dot product for that digit will be which makes the model more likely to predict that digit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4037824",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "d4037824",
        "outputId": "1ab2b702-9d17-4189-b77e-d078e9575ad9"
      },
      "outputs": [],
      "source": [
        "# Plot logistic regression weights before and after training\n",
        "fig1, ax1 = plt.subplots(2, 10, figsize=(14, 2.6))\n",
        "for digit in range(10):\n",
        "    ax1[0, digit].imshow(np.reshape(weights_before[digit, :], newshape=(28, 28)), cmap='gray')\n",
        "    ax1[1, digit].imshow(np.reshape(weights_after[digit, :], newshape=(28, 28)), cmap='gray')\n",
        "    ax1[0, digit].axis('off')\n",
        "    ax1[1, digit].axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cabe4c92",
      "metadata": {
        "id": "cabe4c92"
      },
      "source": [
        "## **5. Evaluating Logistic Regression Model on Test Set**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78946397",
      "metadata": {
        "id": "78946397"
      },
      "source": [
        "### 5.1. Evaluate model on test set\n",
        "\n",
        "Once our model has been trained, we now need to evaluate it on a set of hold-out data, or a test set. Evaluating trained models on a test set is a critical step of machine learning pipelines, as this gives us an idea of how well our model will generalize in the real world to new, never-seen-before data.\n",
        "\n",
        "First, we need to put our model in evaluation mode using the `eval()` method. This method affects operations with different training and test-time behavior such as dropout and batch normalization. Although we don't use any layers in this tutorial that would be affected by this method, it's a good thing to get into the habit of doing.\n",
        "\n",
        "Additionally, since we aren't training our model any more via backpropagation, we don't need to compute a loss or calculate gradients. We can use the [`torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html) context-manager that disables gradient calculation. Disabling gradient calculation is useful for model inference, when you are sure that you will not call Tensor.backward(). Using `torch.no_grad()` will help reduce memory consumption for computations that would otherwise require gradients during training [6].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92af02be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92af02be",
        "outputId": "ef726323-05e5-4874-aa86-2d63e4094328"
      },
      "outputs": [],
      "source": [
        "# Place model in evaluation mode\n",
        "# .eval() method affects operations such as dropout and batch normalization\n",
        "lr_model.eval()\n",
        "\n",
        "# Initialize test set metrics variables\n",
        "n_obs = 0\n",
        "n_correct = 0\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Iterate through test data mini-batches\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        # Forward pass\n",
        "        y_batch = y_batch.squeeze(1)\n",
        "        y_pred = lr_model(X_batch)  # model predictions\n",
        "\n",
        "        # Update test set metrics variables\n",
        "        n_batch = len(y_batch)\n",
        "        n_obs += n_batch\n",
        "        n_correct += (y_pred.argmax(1) == y_batch.long()).float().sum().item()\n",
        "\n",
        "# Calculate test accuracy\n",
        "lr_acc_test = n_correct / n_obs\n",
        "\n",
        "# Display test accuracy\n",
        "print('\\nLogistic regression model MNIST test acc.:  %.4f\\n' % (lr_acc_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39def432",
      "metadata": {
        "id": "39def432"
      },
      "source": [
        "## **6. Training and Evaluating Multi-Layer Perceptron on MNIST Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8419fab",
      "metadata": {
        "id": "f8419fab"
      },
      "source": [
        "### 6.1. Defining MLP model\n",
        "\n",
        "Similarly to how we previously defined our logistic regression model by using PyTorch's object-oriented style, we can implement a neural network/[multi-layer perceptron](https://machinelearningmastery.com/neural-networks-crash-course/) (MLP) with just a few tweaks. Instead of just one `Linear` layer, this time we'll use two. Additionally, after the first `Linear` layer we'll apply a non-linear [activation function](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/). Activation functions are a critical part of neural network design, and help give our neural network model added expressivity for modelling complex, non-linear relationships. The de facto nonlinearity used in neural networks is the [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (ReLU). We can implement this ReLU activation function as a layer in our MLP by using PyTorch's [`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) layer.\n",
        "\n",
        "\n",
        "*Note: multi-layer perceptron and artifical neural network (ANN) are often used interchangeably to describe a feed-forward, fully-connected neural network with many layers.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c36db77",
      "metadata": {
        "id": "1c36db77"
      },
      "outputs": [],
      "source": [
        "# Multi-layer perceptron model class definition\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of a multi-layer perceptron model.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dim : int\n",
        "        Size/dimensionality of the model input data.\n",
        "    output_dim : int\n",
        "        Size/dimensionality of the model output.\n",
        "    hidden_dim : int\n",
        "        Size/dimensionality of the hidden layer.\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    input_dim : int\n",
        "        Size/dimensionality of the model input data.\n",
        "    output_dim : int\n",
        "        Size/dimensionality of the model output.\n",
        "    hidden_dim : int\n",
        "        Size/dimensionality of the hidden layer.\n",
        "    linear_1 : torch.nn.Linear\n",
        "        Fully-connected/linear neural network layer.\n",
        "    linear_2 : torch.nn.Linear\n",
        "        Fully-connected/linear neural network layer.\n",
        "    relu : torch.nn.ReLU\n",
        "        Rectified linear unit activation function.\n",
        "    \"\"\"\n",
        "    \n",
        "    # MLP instantiation method\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=100):\n",
        "        \"\"\"\n",
        "        Model instantiation method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Size/dimensionality of the model input data.\n",
        "        output_dim : int\n",
        "            Size/dimensionality of the model output.\n",
        "        hidden_dim : int\n",
        "            Size/dimensionality of the hidden layer.\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        input_dim : int\n",
        "            Size/dimensionality of the model input data.\n",
        "        output_dim : int\n",
        "            Size/dimensionality of the model output.\n",
        "        hidden_dim : int\n",
        "            Size/dimensionality of the hidden layer.\n",
        "        linear_1 : torch.nn.Linear\n",
        "            Fully-connected/linear neural network layer.\n",
        "        linear_2 : torch.nn.Linear\n",
        "            Fully-connected/linear neural network layer.\n",
        "        relu : torch.nn.ReLU\n",
        "            Rectified linear unit activation function.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Inherit from torch.nn.Module\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        # Assign MLP parameters to model attributes\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        # Define MLP layers\n",
        "        self.linear_1 = nn.Linear(\n",
        "            in_features=input_dim,\n",
        "            out_features=hidden_dim,\n",
        "            bias=True)\n",
        "        self.linear_2 = nn.Linear(\n",
        "            in_features=hidden_dim,\n",
        "            out_features=output_dim,\n",
        "            bias=True)\n",
        "        \n",
        "        # Define MLP activation function\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    # MLP forward pass method\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        MLP forward pass method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor()\n",
        "            Tensor of input data.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        logits : torch.Tensor()\n",
        "             Raw model predictions (not passed through sigmoid or softmax function).\n",
        "        \"\"\"\n",
        "        \n",
        "        # Forward pass through model\n",
        "        x = self.linear_1(x)       # first fully-connected layer\n",
        "        x = self.relu(x)           # non-linear activation transformation\n",
        "        logits = self.linear_2(x)  # second fully-connected layer\n",
        "        \n",
        "        # Return model output\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60789485",
      "metadata": {
        "id": "60789485"
      },
      "source": [
        "### 6.2. Instantiate MLP model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b68a77",
      "metadata": {
        "id": "c8b68a77"
      },
      "outputs": [],
      "source": [
        "# Define model input and output dimensions from data\n",
        "input_dim = X_train.shape[1]                  # second dimension is data dimensionality\n",
        "output_dim = len(torch.unique(y_train))  # output dim. is equal to number of unique labels\n",
        "\n",
        "# Instantiate multi-layer perceptron model\n",
        "mlp_model = MLP(input_dim=input_dim, output_dim=output_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0313d255",
      "metadata": {
        "id": "0313d255"
      },
      "source": [
        "### 6.3. Re-define optimizer with MLP model parameters\n",
        "\n",
        "Since a PyTorch optimizer is an object that keeps track of and updates model parameters, we need to re-define our optimizer with the parameters of our newly defined MLP model. We can keep the same learning rate of 0.1 as last time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc50caab",
      "metadata": {
        "id": "fc50caab"
      },
      "outputs": [],
      "source": [
        "# Instantiate stochastic gradient descent (SGD) optimizer\n",
        "lr = 0.1\n",
        "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c218673",
      "metadata": {
        "id": "2c218673"
      },
      "source": [
        "### 6.4. Train MLP model on training data\n",
        "\n",
        "Now we train our MLP model on MNIST. Ideally, since we have now defined a non-linear model with added expressivity by including a hidden layer and a non-linear activation function following this hidden layer, we would like to see better classification performance than that achieved by logistic regression. As we will see, these modifications do indeed result in better hand-written digit classification performance.\n",
        "\n",
        "Even though we changed the type of model, we can keep the same exact training loop we developed for training logistic regression on MNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c95aecf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c95aecf",
        "outputId": "a370f77c-4302-4c01-ce25-7cb3f3c0f02e"
      },
      "outputs": [],
      "source": [
        "# Number of training epochs (epochs = # of passes through data)\n",
        "n_epochs = 20\n",
        "\n",
        "# Time model training\n",
        "print('\\nTraining MLP model...\\n')\n",
        "time_start = time.time()\n",
        "\n",
        "# Place model in training mode\n",
        "# .train() method affects operations such as dropout and batch normalization\n",
        "mlp_model.train()\n",
        "\n",
        "# Train model/iterate over epochs\n",
        "for epoch in range(n_epochs):\n",
        "    # Initialize epoch metrics variables\n",
        "    n_obs = 0\n",
        "    loss_sum = 0\n",
        "    n_correct = 0\n",
        "    \n",
        "    # Iterate through training data mini-batches\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # Zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        y_batch = y_batch.squeeze(1)\n",
        "        y_pred = mlp_model(X_batch)         # model predictions\n",
        "        loss = loss_func(y_pred, y_batch)  # loss function evaluation\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()    # backpropagation\n",
        "        optimizer.step()   # update parameters according to learning rate, gradients\n",
        "        \n",
        "        # Update epoch metrics variables\n",
        "        n_batch = len(y_batch)\n",
        "        n_obs += n_batch\n",
        "        loss_sum += n_batch * loss.item()\n",
        "        n_correct += (y_pred.argmax(1) == y_batch.long()).float().sum().item()\n",
        "    \n",
        "    # Calculate epoch training loss and training accuracy\n",
        "    loss_train = loss_sum / n_obs\n",
        "    acc_train = n_correct / n_obs\n",
        "    \n",
        "    # Display training progress\n",
        "    prog_disp_freq = 1   # frequency of training progress display\n",
        "    if (epoch + 1) % prog_disp_freq == 0 or epoch == 0 or epoch + 1 == n_epochs:\n",
        "        print('  E%02d | train loss: %s | train acc.: %s' % \n",
        "              (epoch + 1, '{:.4f}'.format(loss_train), '{:.4f}'.format(acc_train)))\n",
        "\n",
        "# Time model training\n",
        "time_end = time.time()\n",
        "print('\\nMLP model training complete.\\n')\n",
        "print('Time to train MLP model:  %.1f s\\n' % (time_end - time_start))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56e1b4c3",
      "metadata": {
        "id": "56e1b4c3"
      },
      "source": [
        "### 6.5. Evaluate MLP model on test set\n",
        "\n",
        "We'll now evaluate our MLP model on the test set. The added expressivity and complexity of our model can be a double-edged sword. While this added complexity gives our model the capacity to learn more complex, non-linear relationships, we must now also be weary of [overfitting](https://www.ibm.com/cloud/learn/overfitting). Overfitting occurs when a model learns the information represented in training data to the point where it's performance on hold-out data is negatively influenced. Deep learning models can have millions (sometimes [billions](https://developer.nvidia.com/blog/openai-presents-gpt-3-a-175-billion-parameters-language-model/)!) of trainable parameters, which means that the number of model parameters can be greater than the number of data points on which the model is trained. These \"over-parametrized\" models can be prone to overfitting, especially in cases of small data. Here, we're hoping that our test accuracy is similar to our training accuracy. As we'll see, our model did not overfit the training set and achieves a comparable test set accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42236f0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42236f0a",
        "outputId": "deef7ab9-542a-402d-9d90-4b8c504c9435"
      },
      "outputs": [],
      "source": [
        "# Place model in evaluation mode\n",
        "# .eval() method affects operations such as dropout and batch normalization\n",
        "mlp_model.eval()\n",
        "\n",
        "# Initialize test set metrics variables\n",
        "n_obs = 0\n",
        "n_correct = 0\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Iterate through test data mini-batches\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        # Forward pass\n",
        "        y_batch = y_batch.squeeze(1)\n",
        "        y_pred = mlp_model(X_batch)  # model predictions\n",
        "\n",
        "        # Update test set metrics variables\n",
        "        n_batch = len(y_batch)\n",
        "        n_obs += n_batch\n",
        "        n_correct += (y_pred.argmax(1) == y_batch.long()).float().sum().item()\n",
        "\n",
        "# Calculate test accuracy\n",
        "mlp_acc_test = n_correct / n_obs\n",
        "\n",
        "# Display test accuracy\n",
        "print('\\nMLP model MNIST test acc.:  %.4f' % (mlp_acc_test))\n",
        "print('\\nLogistic Regression model MNIST test acc.:  %.4f\\n' % (lr_acc_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e516c5",
      "metadata": {
        "id": "03e516c5"
      },
      "source": [
        "## **References**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d3a6b3",
      "metadata": {
        "id": "e5d3a6b3"
      },
      "source": [
        "[1] https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
        "\n",
        "[2] https://pytorch.org/docs/stable/tensor_view.html\n",
        "\n",
        "[3] https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "[4] https://pytorch.org/docs/stable/notes/modules.html\n",
        "\n",
        "[5] https://pytorch.org/docs/stable/autograd.html, https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "[6] https://pytorch.org/docs/stable/generated/torch.no_grad.html\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
